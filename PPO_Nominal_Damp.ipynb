{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "800e2b58",
   "metadata": {},
   "source": [
    "# PPO for Quantum system control\n",
    "In this simulation we are going to test the PPO alorithm in a simulation of a quantum environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55824490",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b0e933",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import gym, scipy.linalg, tensorboard\n",
    "import qiskit\n",
    "import qiskit.quantum_info as qi\n",
    "from qiskit.quantum_info.states import DensityMatrix\n",
    "from qiskit.quantum_info import state_fidelity\n",
    "import math\n",
    "import numpy as np\n",
    "from gym.spaces.box import Box\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_theme()\n",
    "import pandas as pd\n",
    "import os\n",
    "import optuna\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322563d8",
   "metadata": {},
   "source": [
    "## Define the enviroment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dd5c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quantum_dynamics(gym.Env):\n",
    "\n",
    "    MAX_STEPS = 500\n",
    "    FIDELITY_THRESHOLD = 0.97\n",
    "\n",
    "    def __init__(self, env_config):\n",
    "\n",
    "        #Define the action and observation spaces\n",
    "        self.action_space = Box(-1,1,shape=(1,), dtype=np.float64)\n",
    "        self.observation_space = Box(low=-1, high=1, shape=(3, 3), dtype=np.float64)\n",
    "\n",
    "        #Define the epsilon variable for the measurement \n",
    "        self.eps = env_config[\"eps\"]\n",
    "        ##Define the probability of the noise\n",
    "        self.gamma = env_config[\"alpha\"]\n",
    "        gamma_1 = 0\n",
    "        gamma_2 = self.gamma/2\n",
    "        gamma_3 = self.gamma/2\n",
    "        #Define the target state\n",
    "        self.rho_target = env_config[\"target\"]\n",
    "\n",
    "        #Define the measurement operators\n",
    "        self.meas_operators = {\n",
    "                                \"M0\": np.matrix([[np.sqrt(1-2*(self.eps)), 0, 0], [0, np.sqrt(self.eps), 0], [0, 0, np.sqrt(self.eps)]]),\n",
    "                                \"M1\": np.matrix([[np.sqrt(self.eps), 0, 0], [0, np.sqrt(1-2*(self.eps)), 0], [0, 0, np.sqrt(self.eps)]]),\n",
    "                                \"M2\": np.matrix([[np.sqrt(self.eps), 0, 0], [0, np.sqrt(self.eps), 0], [0, 0, np.sqrt(1-2*(self.eps))]]),\n",
    "                              }\n",
    "        self.meas_outcomes = np.array([0,1,2])\n",
    "        \n",
    "        #Define flip channel kraus representation \n",
    "        self.K = {\"k0\": np.matrix([[1,0,0], [0, np.sqrt(1-gamma_1), 0], [0, 0, np.sqrt(1-gamma_1-gamma_2)]]),\n",
    "                  \"k01\": np.matrix([[0,np.sqrt(gamma_1),0], [0, 0, 0], [0, 0, 0]]),\n",
    "                  \"k03\": np.matrix([[0,0,0], [0, 0, 0], [0, 0, gamma_3]]),\n",
    "                  \"k12\": np.matrix([[0, 0, 0], [0, 0, np.sqrt(gamma_2)], [0, 0, 0]])}\n",
    "        \n",
    "        #Hamiltonian components\n",
    "        self.a = np.matrix([[0,1,0], [0,0,1], [0,0,0]])\n",
    "\n",
    "    def reset(self):\n",
    "        self.rho = qi.random_density_matrix(3)\n",
    "        self.rho_hat = self.rho\n",
    "        self.true_fidelity = 0\n",
    "        self.fidelity = 0\n",
    "        self.count = 0\n",
    "        self.reward = 0\n",
    "        self.done = False\n",
    "        self.info = {}\n",
    "\n",
    "        return self.rho\n",
    "\n",
    "    def step(self, action):\n",
    "        #Check the state of the episode\n",
    "        if self.done:\n",
    "            # should never reach this point\n",
    "            print(\"EPISODE DONE!!!\")\n",
    "        elif (self.count == self.MAX_STEPS):\n",
    "            self.done = True;\n",
    "        else:\n",
    "            assert self.action_space.contains(action)\n",
    "            self.count += 1\n",
    "\n",
    "    \n",
    "        #Compute the effect of the noise\n",
    "        rho = np.matrix(self.rho)\n",
    "        rho_prime = np.zeros((3,3)).astype(complex)\n",
    "        for k_i in self.K.values():\n",
    "            rho_prime += k_i*rho*k_i.getH()\n",
    "        \n",
    "        self.rho = rho_prime\n",
    "        self.rho = DensityMatrix(self.rho) \n",
    "        self.rho_hat = DensityMatrix(self.rho_hat)\n",
    "            \n",
    "            \n",
    "        #Define the effect of the unitary operator\n",
    "        self.beta = action[0]\n",
    "        a_dagger = np.transpose(np.conjugate(self.a))\n",
    "        self.control_hamiltonian = (1j)*np.matrix(self.beta*a_dagger - np.conj(self.beta)*self.a)\n",
    "        self.u = np.matrix(scipy.linalg.expm((-1j)*self.control_hamiltonian))\n",
    "        op = qi.Operator(self.u)\n",
    "\n",
    "        # rho prime --> rho after the impulse control\n",
    "        self.rho = self.rho.evolve(op)\n",
    "        self.rho_hat = self.rho_hat.evolve(op)\n",
    "\n",
    "        #Compute the measurement\n",
    "        outcome_true = self.measurement_true()\n",
    "\n",
    "        #Compute the reward\n",
    "        fid = self.compute_fidelity()\n",
    "        self.fidelity = fid[0]\n",
    "        self.true_fidelity = fid[1]\n",
    "        if self.done:\n",
    "            self.reward = self.fidelity\n",
    "        else:\n",
    "            self.reward = 0\n",
    "        \n",
    "        self.info = {\"Fidelity\": self.true_fidelity}\n",
    "        #try:\n",
    "        #    assert self.observation_space.contains(self.rho)\n",
    "        #except AssertionError:\n",
    "        #    print(\"INVALID STATE\", self.rho)\n",
    "        return [self.rho_hat, self.reward, self.done, self.info]\n",
    "    \n",
    "    def measurement_true(self):\n",
    "\n",
    "        #Initialize the probabilities\n",
    "        self.outcomes_probabilities = np.array([1/3, 1/3, 1/3])\n",
    "\n",
    "        #Compute the probabilities for each m outcome\n",
    "        for m, M_m in enumerate(self.meas_operators.values()):\n",
    "            M_m_dagger = np.transpose(np.conjugate(M_m))\n",
    "            self.outcomes_probabilities[m] = np.trace(np.matmul(np.matmul(M_m_dagger,M_m),np.matrix(self.rho)))\n",
    "        add_prob = 1-sum(self.outcomes_probabilities)\n",
    "        self.outcomes_probabilities[1] += add_prob\n",
    "        \n",
    "        #Do the measure\n",
    "        out = np.random.choice(self.meas_outcomes, 1, p=self.outcomes_probabilities)\n",
    "        key_list=list(self.meas_operators.keys())\n",
    "        key = key_list[out[0]]\n",
    "        M_m = self.meas_operators[key]\n",
    "\n",
    "        #Effect of the measurement on the state\n",
    "        self.rho = np.matrix((np.matmul(np.matmul(M_m,np.matrix(self.rho)),M_m.getH())))/np.trace(np.matmul(np.matmul(M_m,np.matrix(self.rho)),M_m.getH()))\n",
    "        self.rho_hat = np.matrix((np.matmul(np.matmul(M_m,np.matrix(self.rho_hat)),M_m.getH())))/np.trace(np.matmul(np.matmul(M_m,np.matrix(self.rho_hat)),M_m.getH()))\n",
    "        return out\n",
    "\n",
    "    def compute_fidelity(self):\n",
    "        rho_hat = DensityMatrix(self.rho_hat)\n",
    "        rho_true = DensityMatrix(self.rho)\n",
    "        target = DensityMatrix(self.rho_target)\n",
    "        fidelity = state_fidelity(rho_hat, target)\n",
    "        true_fidelity = state_fidelity(rho_true, target)\n",
    "        \n",
    "        if fidelity > self.FIDELITY_THRESHOLD:\n",
    "            self.done = True\n",
    "\n",
    "        return fidelity, true_fidelity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32243c09",
   "metadata": {},
   "source": [
    "## Import the PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0764f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45bf984",
   "metadata": {},
   "source": [
    "## Configure the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa69d30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the epsilon for which we have to train a model\n",
    "eps = np.array([0.1, 0.15, 0.175, 0.2, 0.25, 0.3])\n",
    "#Target state\n",
    "rho_target = np.matrix([[0, 0, 0],[0,0,0], [0,0,1]])\n",
    "#Training noise\n",
    "alpha_train = np.array([0])\n",
    "\n",
    "#Define total training timesteps\n",
    "training_steps = 70_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29e706b-de7b-448f-9654-c4d70f891799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(model, env, episodes = 10):\n",
    "    agent = model\n",
    "    fidelity = np.zeros(episodes)\n",
    "    rewards = np.zeros(episodes)\n",
    "    length = np.zeros(episodes)\n",
    "    for ep in range(episodes):\n",
    "        fidelity_ep = 0\n",
    "        length_ep = 0\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        lstm_states = None\n",
    "        num_envs = 1\n",
    "        episode_starts = np.ones((num_envs,), dtype=bool)\n",
    "        while not done:\n",
    "            length_ep +=1\n",
    "            action, lstm_states = agent.predict(obs, state = lstm_states, episode_start=episode_starts, deterministic=True)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_starts = done\n",
    "            if done:\n",
    "                fidelity_ep = info[\"Fidelity\"]\n",
    "                fidelity[ep] = fidelity_ep\n",
    "                length[ep] = length_ep\n",
    "                rewards[ep] = reward\n",
    "    fidelity_mean = np.mean(fidelity)\n",
    "    len_mean = np.mean(length)\n",
    "    reward_mean = np.mean(rewards)\n",
    "    return [fidelity_mean, len_mean, reward_mean, fidelity, length, rewards]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd071be",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Start train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d1bcba",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#We are creating a model for each epsilon that we want to test, the entire training is without noise\n",
    "evaluate_episodes = 10\n",
    "df_train = pd.DataFrame(columns = ['epsilon', 'alpha', 'reward', 'timesteps', 'fidelity'])\n",
    "df_train_mean = pd.DataFrame(columns = ['epsilon', 'alpha', 'reward', 'reward_std', 'timesteps', 'timesteps_std', 'fidelity', 'fidelity_std'])\n",
    "for epsilon in eps:\n",
    "    for alpha in alpha_train:\n",
    "        env_config = {\"target\": rho_target,\n",
    "                 \"eps\": epsilon,\n",
    "                 \"alpha\":alpha}\n",
    "        env = Quantum_dynamics(env_config)\n",
    "        model = PPO(\"MlpPolicy\", env, learning_rate = 3e-4, verbose = 0,\n",
    "                             batch_size = 128 , n_steps=640, ent_coef=0.018, tensorboard_log=\"./Agent2\")\n",
    "\n",
    "        print(\"##### START TRAINING FOR EPSILON = \" + str(epsilon)+ \" , ALPHA =  \"+ str(alpha) + \" #####\")\n",
    "        now = datetime.now()\n",
    "        current_time = now.strftime(\"%H:%M:%S\")\n",
    "        print(\"Starting_time =\", current_time)\n",
    "        model.learn(total_timesteps=training_steps, tb_log_name=\"Agent_eps_\"+str(epsilon)+\"_nominal\")\n",
    "\n",
    "        #Test the agent\n",
    "        mean_fidelity, mean_length, mean_reward, fidelity_per_episode, length_per_episode,reward_per_episode = evaluate_policy(model, env, episodes = evaluate_episodes)\n",
    "\n",
    "        std_fidelity = np.std(fidelity_per_episode)\n",
    "        std_reward = np.std(reward_per_episode)\n",
    "        std_length = np.std(length_per_episode)\n",
    "\n",
    "\n",
    "        df_session = pd.concat([pd.DataFrame([[epsilon,\n",
    "                                               alpha,\n",
    "                                               reward_per_episode[i],\n",
    "                                               length_per_episode[i],\n",
    "                                               fidelity_per_episode[i]]],\n",
    "                                               columns = ['epsilon', 'alpha', 'reward', 'timesteps', 'fidelity']) \n",
    "                                               for i in range(evaluate_episodes)],\n",
    "                                            ignore_index = True)\n",
    "        df_train = pd.concat([df_train, df_session], ignore_index = True)\n",
    "\n",
    "        df_session_mean = pd.concat([pd.DataFrame([[epsilon,\n",
    "                                                    alpha,\n",
    "                                                    mean_reward,\n",
    "                                                    std_reward,\n",
    "                                                    mean_length,\n",
    "                                                    std_length,\n",
    "                                                    mean_fidelity,\n",
    "                                                    std_fidelity]],\n",
    "                                                    columns = ['epsilon', 'alpha', 'reward', 'reward_std', 'timesteps', 'timesteps_std', 'fidelity', 'fidelity_std'])],\n",
    "                                              ignore_index = True)\n",
    "\n",
    "        df_train_mean = pd.concat([df_train_mean, df_session_mean], ignore_index = True)\n",
    "\n",
    "\n",
    "        print(\"## EVALUATE REWARD FOR EPSILON = \" + str(epsilon)+\": mean reward: \" + str(mean_reward) +\": std reward: \" +\n",
    "             str(std_reward) + \" #####\")\n",
    "        print(\"## EVALUATE REAL FIDELITY FOR EPSILON = \" + str(epsilon) + \": mean fidelity: \" + str(mean_fidelity) +\": std fidelity: \" +\n",
    "             str(std_fidelity) +\" #####\")\n",
    "        print(\"## EVALUATE LENGTH FOR EPSILON = \" + str(epsilon)+\": mean length: \" + str(mean_length) +\": std length: \" +\n",
    "             str(std_length) + \" #####\")\n",
    "\n",
    "        # Save a trained model to a file\n",
    "        print(\"--> Saving the model\")\n",
    "        model.save(\"Agent_eps_\"+str(epsilon)+\"_nominal\")\n",
    "        print(\"--> Model saved in  a file\")\n",
    "\n",
    "        now = datetime.now()\n",
    "        current_time = now.strftime(\"%H:%M:%S\")\n",
    "        print(\"Ending time =\", current_time)\n",
    "\n",
    "        print(\"##### END TRAINING #####\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a93bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44776f0",
   "metadata": {},
   "source": [
    "## Test the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa5d220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(epsilon_test, alpha_test, target = np.matrix([[0, 0, 0],[0,0,0], [0,0,1]]),  episodes = 100):\n",
    "    df_test = pd.DataFrame(columns = ['epsilon', 'alpha', 'reward', 'timesteps', 'fidelity'])\n",
    "    df_test_mean = pd.DataFrame(columns = ['epsilon', 'alpha', 'reward', 'reward_std', 'timesteps', 'timesteps_std', 'fidelity', 'fidelity_std'])\n",
    "    evaluate_episodes = episodes\n",
    "    for epsilon in epsilon_test:\n",
    "        for alpha in alpha_test:\n",
    "            #Configure the environement for the test\n",
    "            env_config = {\"target\": target,\n",
    "                 \"eps\": epsilon,\n",
    "                 \"alpha\":alpha}\n",
    "            #Build the environment\n",
    "            env = Quantum_dynamics(env_config)\n",
    "            #Load the trained agent\n",
    "            agent = PPO.load(\"Agent_eps_\"+str(epsilon)+\"_nominal\")\n",
    "            print(\"##### START TEST FOR AGENT EPSILON: \" + str(epsilon)+ \" ALPHA: \"+ str(alpha) + \" #####\")\n",
    "            now = datetime.now()\n",
    "            current_time = now.strftime(\"%H:%M:%S\")\n",
    "            print(\"Starting_time =\", current_time)\n",
    "            \n",
    "            #Test the agent\n",
    "            mean_fidelity, mean_length, mean_reward, fidelity_per_episode, length_per_episode, reward_per_episode = evaluate_policy(agent, env, episodes)\n",
    "\n",
    "            std_fidelity = np.std(fidelity_per_episode)\n",
    "            std_reward = np.std(reward_per_episode)\n",
    "            std_length = np.std(length_per_episode)\n",
    "            #percentage_solved_episodes = sum(reward > 0.95 for reward in reward_per_episode)\n",
    "            #percentage_solved_episodes = percentage_solved_episodes/evaluate_episodes\n",
    "            \n",
    "            #Create the dataframe\n",
    "            df_session = pd.concat([pd.DataFrame([[epsilon,\n",
    "                                                alpha,\n",
    "                                                reward_per_episode[i],\n",
    "                                                length_per_episode[i],\n",
    "                                                fidelity_per_episode[i]]],\n",
    "                                                columns = ['epsilon', 'alpha', 'reward', 'timesteps', 'fidelity']) \n",
    "                                                for i in range(evaluate_episodes)],\n",
    "                                      ignore_index = True)\n",
    "            df_test = pd.concat([df_test, df_session], ignore_index = True)\n",
    "            \n",
    "            df_session_mean = pd.concat([pd.DataFrame([[epsilon,\n",
    "                                                    alpha,\n",
    "                                                    mean_reward,\n",
    "                                                    std_reward,\n",
    "                                                    mean_length,\n",
    "                                                    std_length,\n",
    "                                                    mean_fidelity,\n",
    "                                                    std_fidelity]],\n",
    "                                                    columns = ['epsilon', 'alpha', 'reward', 'reward_std', 'timesteps', 'timesteps_std', 'fidelity', 'fidelity_std'])],\n",
    "                                          ignore_index = True)\n",
    "            \n",
    "            df_test_mean = pd.concat([df_test_mean, df_session_mean], ignore_index = True)\n",
    "\n",
    "            \n",
    "            print(\"#####-----TEST RESULTS FOR \"+ str(epsilon)+ \" AND ALPHA \"+ str(alpha)+\"-----#####\")\n",
    "            print(\"## EVALUATE REWARD: mean reward: \" + str(mean_reward) +\": std reward: \" +\n",
    "             str(std_reward) + \" #####\")\n",
    "            print(\"## EVALUATE REAL FIDELITY: mean fidelity: \" + str(mean_fidelity) +\": std fidelity: \" +\n",
    "             str(std_fidelity) +\" #####\")\n",
    "            print(\"## EVALUATE LENGTH: mean length: \" + str(mean_length) +\": std length: \" +\n",
    "             str(std_length) + \" #####\")\n",
    "            \n",
    "            now = datetime.now()\n",
    "            current_time = now.strftime(\"%H:%M:%S\")\n",
    "            print(\"Ending time =\", current_time)\n",
    "\n",
    "            print(\"##### END TEST #####\")\n",
    "            \n",
    "    return df_test, df_test_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7858b047",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define the alpha for the test\n",
    "alpha_test = np.array([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])\n",
    "evaluate_episodes = 100\n",
    "df_test, df_test_mean = test_agent(epsilon_test = eps, alpha_test = alpha_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0913cb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98a463d-8b40-4188-9208-bc4952f40b57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Visualize the results of the test\n",
    "for i, epsilon in enumerate(eps):\n",
    "    i = i+1\n",
    "    df_eps = df_test[(i-1)*evaluate_episodes*len(alpha_test):(i)*evaluate_episodes*len(alpha_test)]\n",
    "    fig = px.box(df_eps, x=\"alpha\", y=\"fidelity\", points=\"all\", title=\"Fidelity distribution test session with epsilon: \"+\n",
    "                str(epsilon))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efb4cce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Visualize the results of the test\n",
    "for i, epsilon in enumerate(eps):\n",
    "    i = i+1\n",
    "    df_eps = df_test[(i-1)*evaluate_episodes*len(alpha_test):(i)*evaluate_episodes*len(alpha_test)]\n",
    "    fig = px.box(df_eps, x=\"alpha\", y=\"reward\", points=\"all\", title=\"Reward distribution test session with epsilon: \"+\n",
    "                str(epsilon))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5346048",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Visualize the results of the test\n",
    "for i, epsilon in enumerate(eps):\n",
    "    i = i+1\n",
    "    df_eps = df_test[(i-1)*evaluate_episodes*len(alpha_test):(i)*evaluate_episodes*len(alpha_test)]\n",
    "    fig = px.box(df_eps, x=\"alpha\", y=\"timesteps\", points=\"all\", title=\"Timesteps distribution test session with epsilon: \"+\n",
    "                str(epsilon))\n",
    "    fig.update_traces(orientation='v')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7db382e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df_test_mean, x='alpha', y='fidelity', color='epsilon', symbol=\"epsilon\",\n",
    "             title = 'Fidelity in function of Noise')\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(df_test_mean, x='alpha', y='reward', color='epsilon', symbol=\"epsilon\",\n",
    "             title = 'Reward in function of Noise')\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(df_test_mean, x='alpha', y='timesteps', color='epsilon', symbol=\"epsilon\",\n",
    "             title = 'Timesteps in function of Noise')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a52104-6e22-422c-90e4-02af5a558e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_table = pd.pivot_table(df_test_mean, values='fidelity', index=['epsilon'],\n",
    "                    columns=['alpha'])\n",
    "\n",
    "sns.set(rc={\"figure.figsize\":(12, 6)}) #width=8, height=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec920adb-38d2-42e6-89b0-4895d15a85f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(heatmap_table, cbar_kws={'label': 'Fidelity Real'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7273edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_table = pd.pivot_table(df_test_mean, values='reward', index=['epsilon'],\n",
    "                    columns=['alpha'])\n",
    "\n",
    "sns.set(rc={\"figure.figsize\":(12, 6)}) #width=8, height=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92593808",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(heatmap_table, cbar_kws={'label': 'Fidelity Filtered'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3853cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_table = pd.pivot_table(df_test_mean, values='timesteps', index=['epsilon'],\n",
    "                    columns=['alpha'])\n",
    "\n",
    "sns.set(rc={\"figure.figsize\":(12, 6)}) #width=8, height=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3876e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(heatmap_table, cmap=\"Blues\", cbar_kws={'label': 'Timesteps'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a55783",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('data2/df_test', exist_ok=True)  \n",
    "os.makedirs('data2/df_test_mean', exist_ok=True)  \n",
    "df_test.to_csv('data2/df_test/data.csv') \n",
    "df_test_mean.to_csv('data2/df_test_mean/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276087bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
