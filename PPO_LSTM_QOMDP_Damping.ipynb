{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "800e2b58",
   "metadata": {
    "id": "800e2b58"
   },
   "source": [
    "# Recurrent PPO for quantum control, Model Free\n",
    "In this simulation we are going to test the PPO alorithm in a simulation of a quantum environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55824490",
   "metadata": {
    "id": "55824490"
   },
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b0e933",
   "metadata": {
    "id": "f6b0e933"
   },
   "outputs": [],
   "source": [
    "import gym, scipy.linalg, tensorboard\n",
    "import qiskit\n",
    "import qiskit.quantum_info as qi\n",
    "from qiskit.quantum_info.states import DensityMatrix\n",
    "from qiskit.quantum_info import state_fidelity\n",
    "import math\n",
    "import numpy as np\n",
    "from gym.spaces.box import Box\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_theme()\n",
    "import pandas as pd\n",
    "import os\n",
    "import optuna\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322563d8",
   "metadata": {
    "id": "322563d8"
   },
   "source": [
    "## Define the enviroment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ccd972",
   "metadata": {
    "id": "36ccd972"
   },
   "source": [
    "In this environment we will return as state the control and the observation. Due to this the control will start at time 1 and not at time 0. In this environment we keep as reward the fidelity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dd5c59",
   "metadata": {
    "id": "91dd5c59"
   },
   "outputs": [],
   "source": [
    "class Quantum_dynamics(gym.Env):\n",
    "\n",
    "    MAX_STEPS = 500\n",
    "    FIDELITY_THRESHOLD = 0.95\n",
    "\n",
    "    def __init__(self, env_config):\n",
    "\n",
    "        #Define the action and observation spaces\n",
    "        self.action_space = Box(-1,1,shape=(1,), dtype=np.float64)\n",
    "        self.observation_space = Box(low=-1, high=1, shape=(3, 3), dtype=np.float64)\n",
    "\n",
    "        #Define the epsilon variable for the measurement \n",
    "        self.eps = env_config[\"eps\"]\n",
    "        ##Define the probability of the noise\n",
    "        self.gamma = env_config[\"alpha\"]\n",
    "        gamma_1 = 0\n",
    "        gamma_2 = self.gamma/2\n",
    "        gamma_3 = self.gamma/2\n",
    "        #Define the target state\n",
    "        self.rho_target = env_config[\"target\"]\n",
    "\n",
    "        #Define the measurement operators\n",
    "        self.meas_operators = {\n",
    "                                \"M0\": np.matrix([[np.sqrt(1-2*(self.eps)), 0, 0], [0, np.sqrt(self.eps), 0], [0, 0, np.sqrt(self.eps)]]),\n",
    "                                \"M1\": np.matrix([[np.sqrt(self.eps), 0, 0], [0, np.sqrt(1-2*(self.eps)), 0], [0, 0, np.sqrt(self.eps)]]),\n",
    "                                \"M2\": np.matrix([[np.sqrt(self.eps), 0, 0], [0, np.sqrt(self.eps), 0], [0, 0, np.sqrt(1-2*(self.eps))]]),\n",
    "                              }\n",
    "        self.meas_outcomes = np.array([0,1,2])\n",
    "        \n",
    "        self.projective_oper = {\n",
    "                                \"M0\": np.matrix([[1, 0, 0], [0, 0, 0], [0, 0, 0]]),\n",
    "                                \"M1\": np.matrix([[0, 0, 0], [0, 1, 0], [0, 0, 0]]),\n",
    "                                \"M2\": np.matrix([[0, 0, 0], [0, 0, 0], [0, 0, 1]]),\n",
    "                              }\n",
    "        \n",
    "        #Define flip channel kraus representation \n",
    "        self.K = {\"k0\": np.matrix([[1,0,0], [0, np.sqrt(1-gamma_1), 0], [0, 0, np.sqrt(1-gamma_1-gamma_2)]]),\n",
    "                  \"k01\": np.matrix([[0,np.sqrt(gamma_1),0], [0, 0, 0], [0, 0, 0]]),\n",
    "                  \"k03\": np.matrix([[0,0,0], [0, 0, 0], [0, 0, gamma_3]]),\n",
    "                  \"k12\": np.matrix([[0, 0, 0], [0, 0, np.sqrt(gamma_2)], [0, 0, 0]])}\n",
    "        \n",
    "        #Hamiltonian components\n",
    "        self.a = np.matrix([[0,1,0], [0,0,1], [0,0,0]])\n",
    "        \n",
    "    def reset(self):\n",
    "        self.rho = qi.random_density_matrix(3)\n",
    "        self.fidelity = 0\n",
    "        self.count = 0\n",
    "        self.reward = 0\n",
    "        self.done = False\n",
    "        self.info = {}\n",
    "        \n",
    "        #Compute the measurement\n",
    "        outcome = self.measurement()\n",
    "\n",
    "        # Initialization of the state as vector with the 1st entry --> control, 2nd entry -->outcome\n",
    "        self.state = np.array([0])\n",
    "        self.state = np.append(self.state, outcome)\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        if action[1] < 0:\n",
    "            self.done = 0\n",
    "        else:\n",
    "            self.done = 1\n",
    "        if self.done:\n",
    "            self.beta = action[0]\n",
    "            outcome = self.measurement()\n",
    "            if outcome == 2:\n",
    "                self.reward = 1\n",
    "            else:\n",
    "                self.reward = -1\n",
    "                \n",
    "            #Compute the reward\n",
    "            self.fidelity = self.compute_fidelity()\n",
    "            \n",
    "            self.info = {\"Fidelity\": self.fidelity}\n",
    "            \n",
    "            #Build the state of the agent\n",
    "            self.state[0] = self.beta\n",
    "            self.state[1] = outcome\n",
    "            \n",
    "            return [self.state, self.reward, True, self.info]\n",
    "            \n",
    "        elif (self.count == self.MAX_STEPS):\n",
    "            self.done = True;\n",
    "            self.reward = -1\n",
    "            \n",
    "            #Compute the reward\n",
    "            self.fidelity = self.compute_fidelity()\n",
    "            \n",
    "            self.info = {\"Fidelity\": self.fidelity}\n",
    "            #Build the state of the agent\n",
    "            return [self.state, self.reward, self.done, self.info]\n",
    "        else:\n",
    "            #print(action)\n",
    "            #assert self.action_space.contains(action)\n",
    "            self.count += 1\n",
    "\n",
    "    \n",
    "        #Compute the effect of the noise\n",
    "        rho = self.rho\n",
    "        rho_prime = np.zeros((3,3)).astype(complex)\n",
    "        for k_i in self.K.values():\n",
    "            rho_prime += k_i*rho*k_i.getH()\n",
    "            \n",
    "        self.rho = rho_prime\n",
    "        self.rho = DensityMatrix(self.rho) \n",
    "        \n",
    "        #Define the effect of the unitary operator\n",
    "        self.beta = action[0]\n",
    "        a_dagger = np.transpose(np.conjugate(self.a))\n",
    "        self.control_hamiltonian = (1j)*np.matrix(self.beta*a_dagger - np.conj(self.beta)*self.a)\n",
    "        self.u = np.matrix(scipy.linalg.expm((-1j)*self.control_hamiltonian))\n",
    "        op = qi.Operator(self.u)\n",
    "\n",
    "        # rho prime --> rho after the impulse control\n",
    "        self.rho = self.rho.evolve(op)\n",
    "\n",
    "        #Compute the measurement\n",
    "        outcome = self.measurement()\n",
    "        \n",
    "        #Assign the reward\n",
    "        self.reward = 0\n",
    "        \n",
    "        self.info = {\"Fidelity\": 0}\n",
    "        \n",
    "        #Build the state of the agent\n",
    "        self.state[0] = self.beta\n",
    "        self.state[1] = outcome\n",
    "        return [self.state, self.reward, self.done, self.info]\n",
    "    \n",
    "    def measurement(self):\n",
    "\n",
    "        #Initialize the probabilities\n",
    "        self.outcomes_probabilities = np.array([1/3, 1/3, 1/3])\n",
    "\n",
    "        if self.done:\n",
    "            #Compute the probabilities for each m outcome\n",
    "            for m, M_m in enumerate(self.projective_oper.values()):\n",
    "                M_m_dagger = np.transpose(np.conjugate(M_m))\n",
    "                self.outcomes_probabilities[m] = np.trace(M_m_dagger*M_m*np.matrix(self.rho))\n",
    "            add_prob = 1-sum(self.outcomes_probabilities)\n",
    "            self.outcomes_probabilities[1] += add_prob\n",
    "\n",
    "            #Do the measure\n",
    "            try:\n",
    "                out = np.random.choice(self.meas_outcomes, 1, p=self.outcomes_probabilities)\n",
    "            except ValueError:\n",
    "                print(\"Probability error \" + str(self.rho))\n",
    "            \n",
    "        else:\n",
    "            #Compute the probabilities for each m outcome\n",
    "            for m, M_m in enumerate(self.meas_operators.values()):\n",
    "                M_m_dagger = np.transpose(np.conjugate(M_m))\n",
    "                self.outcomes_probabilities[m] = np.trace(M_m_dagger*M_m*np.matrix(self.rho))\n",
    "            add_prob = 1-sum(self.outcomes_probabilities)\n",
    "            self.outcomes_probabilities[1] += add_prob\n",
    "\n",
    "            #Do the measure\n",
    "            try:\n",
    "                out = np.random.choice(self.meas_outcomes, 1, p=self.outcomes_probabilities)\n",
    "            except ValueError:\n",
    "                print(\"Probability error \" + str(self.rho))\n",
    "\n",
    "            key_list=list(self.meas_operators.keys())\n",
    "            key = key_list[out[0]]\n",
    "            M_m = self.meas_operators[key]\n",
    "\n",
    "            #Effect of the measurement on the state\n",
    "            self.rho = np.matrix((M_m*np.matrix(self.rho)*M_m.getH())/np.trace(M_m*np.matrix(self.rho)*M_m.getH()))\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def compute_fidelity(self):\n",
    "        rho = DensityMatrix(self.rho)\n",
    "        target = DensityMatrix(self.rho_target)\n",
    "        fidelity = state_fidelity(rho, target)\n",
    "        \n",
    "        if fidelity > self.FIDELITY_THRESHOLD:\n",
    "            self.done = True\n",
    "\n",
    "        return fidelity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32243c09",
   "metadata": {
    "id": "32243c09"
   },
   "source": [
    "## Import the PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0764f5",
   "metadata": {
    "id": "eb0764f5"
   },
   "outputs": [],
   "source": [
    "from sb3_contrib import RecurrentPPO\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd071be",
   "metadata": {
    "id": "5cd071be"
   },
   "source": [
    "## Start train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b154711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(model, env, episodes = 10):\n",
    "    agent = model\n",
    "    fidelity = np.zeros(episodes)\n",
    "    rewards = np.zeros(episodes)\n",
    "    length = np.zeros(episodes)\n",
    "    for ep in range(episodes):\n",
    "        fidelity_ep = 0\n",
    "        length_ep = 0\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        lstm_states = None\n",
    "        num_envs = 1\n",
    "        episode_starts = np.ones((num_envs,), dtype=bool)\n",
    "        while not done:\n",
    "            length_ep +=1\n",
    "            action, lstm_states = agent.predict(obs, state = lstm_states, episode_start=episode_starts, deterministic=True)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_starts = done\n",
    "            if done:\n",
    "                fidelity_ep = info[\"Fidelity\"]\n",
    "                fidelity[ep] = fidelity_ep\n",
    "                length[ep] = length_ep\n",
    "                rewards[ep] = reward\n",
    "    fidelity_mean = np.mean(fidelity)\n",
    "    len_mean = np.mean(length)\n",
    "    reward_mean = np.mean(rewards)\n",
    "    return [fidelity_mean, len_mean, reward_mean, fidelity, length, rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45634806",
   "metadata": {
    "id": "45634806"
   },
   "outputs": [],
   "source": [
    "# Select the epsilon for which we have to train a model\n",
    "eps = np.array([0.1, 0.15, 0.175, 0.2, 0.25, 0.3])\n",
    "#Target state\n",
    "rho_target = np.matrix([[0, 0, 0],[0,0,0], [0,0,1]])\n",
    "#Training noise\n",
    "alpha_train = 0\n",
    "\n",
    "#Define total training timesteps\n",
    "training_steps = 400_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e42532",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "73e42532",
    "outputId": "b8f9cce5-d24a-4087-ecff-074f80f59e3a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#We are creating a model for each epsilon that we want to test, the entire training is without noise\n",
    "evaluate_episodes = 10\n",
    "df_train = pd.DataFrame(columns = ['epsilon', 'alpha', 'reward', 'timesteps', 'fidelity'])\n",
    "df_train_mean = pd.DataFrame(columns = ['epsilon', 'alpha', 'reward', 'reward_std', 'timesteps', 'timesteps_std', 'fidelity', 'fidelity_std'])\n",
    "for epsilon in eps:\n",
    "    env_config = {\"target\": rho_target,\n",
    "             \"eps\": epsilon,\n",
    "             \"alpha\":alpha_train}\n",
    "    env = Quantum_dynamics(env_config)\n",
    "    model = RecurrentPPO(\"MlpLstmPolicy\", env, learning_rate = 3e-4, verbose = 0,\n",
    "                         batch_size = 512 , n_steps=512, tensorboard_log=\"./QOMDP_Depolarizing\")\n",
    "    \n",
    "    print(\"##### START TRAINING FOR EPSILON = \" + str(epsilon)+ \" #####\")\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(\"Starting_time =\", current_time)\n",
    "    model.learn(total_timesteps=training_steps, tb_log_name=\"LSTM_train_alpha_0_eps\"+str(epsilon*100))\n",
    "    \n",
    "    #Test the agent\n",
    "    mean_fidelity, mean_length, mean_reward, fidelity_per_episode, length_per_episode,reward_per_episode = evaluate_policy(model, env, episodes = evaluate_episodes)\n",
    "\n",
    "    std_fidelity = np.std(fidelity_per_episode)\n",
    "    std_reward = np.std(reward_per_episode)\n",
    "    std_length = np.std(length_per_episode)\n",
    "\n",
    "\n",
    "    df_session = pd.concat([pd.DataFrame([[epsilon,\n",
    "                                           alpha_train,\n",
    "                                           reward_per_episode[i],\n",
    "                                           length_per_episode[i],\n",
    "                                           fidelity_per_episode[i]]],\n",
    "                                           columns = ['epsilon', 'alpha', 'reward', 'timesteps', 'fidelity']) \n",
    "                                           for i in range(evaluate_episodes)],\n",
    "                                        ignore_index = True)\n",
    "    df_train = pd.concat([df_train, df_session], ignore_index = True)\n",
    "            \n",
    "    df_session_mean = pd.concat([pd.DataFrame([[epsilon,\n",
    "                                                alpha_train,\n",
    "                                                mean_reward,\n",
    "                                                std_reward,\n",
    "                                                mean_length,\n",
    "                                                std_length,\n",
    "                                                mean_fidelity,\n",
    "                                                std_fidelity]],\n",
    "                                                columns = ['epsilon', 'alpha', 'reward', 'reward_std', 'timesteps', 'timesteps_std', 'fidelity', 'fidelity_std'])],\n",
    "                                          ignore_index = True)\n",
    "            \n",
    "    df_train_mean = pd.concat([df_train_mean, df_session_mean], ignore_index = True)\n",
    "\n",
    "    \n",
    "    print(\"## EVALUATE REWARD FOR EPSILON = \" + str(epsilon)+\": mean fidelity: \" + str(mean_fidelity) +\": std fidelity: \" +\n",
    "         str(std_fidelity) + \" #####\")\n",
    "    print(\"## EVALUATE LENGTH FOR EPSILON = \" + str(epsilon)+\": mean length: \" + str(mean_length) +\": std length: \" +\n",
    "         str(std_length) + \" #####\")\n",
    "    \n",
    "    # Save a trained model to a file\n",
    "    print(\"--> Saving the model\")\n",
    "    model.save(\"LSTM_eps_\"+str(epsilon*100)+\"_train\")\n",
    "    print(\"--> Model saved in  a file\")\n",
    "\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(\"Ending time =\", current_time)\n",
    "\n",
    "    print(\"##### END TRAINING #####\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25d6760",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44776f0",
   "metadata": {
    "id": "a44776f0"
   },
   "source": [
    "## Test the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa5d220",
   "metadata": {
    "id": "0fa5d220"
   },
   "outputs": [],
   "source": [
    "def test_agent(epsilon_test, alpha_test, target = np.matrix([[0, 0, 0],[0,0,0], [0,0,1]]),  episodes = 100):\n",
    "    df_test = pd.DataFrame(columns = ['epsilon', 'alpha', 'reward', 'timesteps', 'fidelity'])\n",
    "    df_test_mean = pd.DataFrame(columns = ['epsilon', 'alpha', 'reward', 'reward_std', 'timesteps', 'timesteps_std', 'fidelity', 'fidelity_std'])\n",
    "    evaluate_episodes = episodes\n",
    "    for epsilon in epsilon_test:\n",
    "        for alpha in alpha_test:\n",
    "            #Configure the environement for the test\n",
    "            env_config = {\"target\": target,\n",
    "                 \"eps\": epsilon,\n",
    "                 \"alpha\":alpha}\n",
    "            #Build the environment\n",
    "            env = Quantum_dynamics(env_config)\n",
    "            #Load the trained agent\n",
    "            agent = RecurrentPPO.load(\"LSTM_eps_\"+str(epsilon*100)+\"_train\")\n",
    "            print(\"##### START TEST FOR AGENT EPSILON: \" + str(epsilon)+ \" ALPHA: \"+ str(alpha) + \" #####\")\n",
    "            now = datetime.now()\n",
    "            current_time = now.strftime(\"%H:%M:%S\")\n",
    "            print(\"Starting_time =\", current_time)\n",
    "            \n",
    "            #Test the agent\n",
    "            mean_fidelity, mean_length, mean_reward, fidelity_per_episode, length_per_episode, reward_per_episode = evaluate_policy(agent, env, episodes)\n",
    "\n",
    "            std_fidelity = np.std(fidelity_per_episode)\n",
    "            std_reward = np.std(reward_per_episode)\n",
    "            std_length = np.std(length_per_episode)\n",
    "            #percentage_solved_episodes = sum(reward > 0.95 for reward in reward_per_episode)\n",
    "            #percentage_solved_episodes = percentage_solved_episodes/evaluate_episodes\n",
    "            \n",
    "            #Create the dataframe\n",
    "            df_session = pd.concat([pd.DataFrame([[epsilon,\n",
    "                                                alpha,\n",
    "                                                reward_per_episode[i],\n",
    "                                                length_per_episode[i],\n",
    "                                                fidelity_per_episode[i]]],\n",
    "                                                columns = ['epsilon', 'alpha', 'reward', 'timesteps', 'fidelity']) \n",
    "                                                for i in range(evaluate_episodes)],\n",
    "                                      ignore_index = True)\n",
    "            df_test = pd.concat([df_test, df_session], ignore_index = True)\n",
    "            \n",
    "            df_session_mean = pd.concat([pd.DataFrame([[epsilon,\n",
    "                                                    alpha,\n",
    "                                                    mean_reward,\n",
    "                                                    std_reward,\n",
    "                                                    mean_length,\n",
    "                                                    std_length,\n",
    "                                                    mean_fidelity,\n",
    "                                                    std_fidelity]],\n",
    "                                                    columns = ['epsilon', 'alpha', 'reward', 'reward_std', 'timesteps', 'timesteps_std', 'fidelity', 'fidelity_std'])],\n",
    "                                          ignore_index = True)\n",
    "            \n",
    "            df_test_mean = pd.concat([df_test_mean, df_session_mean], ignore_index = True)\n",
    "\n",
    "            \n",
    "            print(\"## TEST RESULTS:  mean fidelity: \" + str(mean_fidelity) +\": std fidelity: \" +\n",
    "                 str(std_fidelity) + \" #####\")\n",
    "            \n",
    "            now = datetime.now()\n",
    "            current_time = now.strftime(\"%H:%M:%S\")\n",
    "            print(\"Ending time =\", current_time)\n",
    "\n",
    "            print(\"##### END TEST #####\")\n",
    "            \n",
    "    return df_test, df_test_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7858b047",
   "metadata": {
    "id": "7858b047",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define the alpha for the test\n",
    "alpha_test = np.array([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])\n",
    "\n",
    "evaluate_episodes = 100\n",
    "\n",
    "df_test, df_test_mean = test_agent(epsilon_test = eps, alpha_test = alpha_test, episodes = evaluate_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0913cb4c",
   "metadata": {
    "id": "0913cb4c"
   },
   "outputs": [],
   "source": [
    "#Visualize the results of the test\n",
    "for i, epsilon in enumerate(eps):\n",
    "    i = i+1\n",
    "    df_eps = df_test[(i-1)*evaluate_episodes*len(alpha_test):(i)*evaluate_episodes*len(alpha_test)]\n",
    "    fig = px.box(df_eps, x=\"alpha\", y=\"fidelity\", points=\"all\", title=\"Fidelity distribution test session with epsilon: \"+\n",
    "                str(epsilon))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5eef1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the results of the test\n",
    "for i, epsilon in enumerate(eps):\n",
    "    i = i+1\n",
    "    df_eps = df_test[(i-1)*evaluate_episodes*len(alpha_test):(i)*evaluate_episodes*len(alpha_test)]\n",
    "    fig = px.box(df_eps, x=\"alpha\", y=\"reward\", points=\"all\", title=\"Rewards distribution test session with epsilon: \"+\n",
    "                str(epsilon))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d92fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the results of the test\n",
    "for i, epsilon in enumerate(eps):\n",
    "    i = i+1\n",
    "    df_eps = df_test[(i-1)*evaluate_episodes*len(alpha_test):(i)*evaluate_episodes*len(alpha_test)]\n",
    "    fig = px.box(df_eps, x=\"alpha\", y=\"timesteps\", points=\"all\", title=\"Timesteps distribution test session with epsilon: \"+\n",
    "                str(epsilon))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7db382e",
   "metadata": {
    "id": "d7db382e",
    "outputId": "06e5bbaf-4844-4c8c-d71c-0df759f2baa5"
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "df_test_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aea161f",
   "metadata": {
    "id": "fdf6d583",
    "outputId": "e3cfa63b-23d3-41cd-e76c-c2e28f5092de"
   },
   "outputs": [],
   "source": [
    "fig = px.line(df_test_mean, x='alpha', y='fidelity', color='epsilon', symbol=\"epsilon\",\n",
    "             title = 'Fidelity in function of Noise')\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(df_test_mean, x='alpha', y='reward', color='epsilon', symbol=\"epsilon\",\n",
    "             title = 'Reward in function of Noise')\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(df_test_mean, x='alpha', y='timesteps', color='epsilon', symbol=\"epsilon\",\n",
    "             title = 'Timesteps in function of Noise')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c72753",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_table = pd.pivot_table(df_test_mean, values='fidelity', index=['epsilon'],\n",
    "                    columns=['alpha'])\n",
    "\n",
    "sns.set(rc={\"figure.figsize\":(12, 6)}) #width=8, height=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe05ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(heatmap_table,  cbar_kws={'label': 'Fidelity'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509ee828",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_table = pd.pivot_table(df_test_mean, values='reward', index=['epsilon'],\n",
    "                    columns=['alpha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bbd032",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(heatmap_table,  cbar_kws={'label': 'Reward'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e0f817",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_table = pd.pivot_table(df_test_mean, values='timesteps', index=['epsilon'],\n",
    "                    columns=['alpha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a68cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(heatmap_table, cbar_kws={'label': 'Timesteps'}, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6847e534",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('data/df_test', exist_ok=True)  \n",
    "os.makedirs('data/df_test_mean', exist_ok=True)  \n",
    "df_test.to_csv('data/df_test/data.csv') \n",
    "df_test_mean.to_csv('data/df_test_mean/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22f22d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
